Subject:
RevBayes report
From:
Fredrik Ronquist <fredrik.ronquist@nrm.se>
Date:
Fri, 12 Feb 2010 13:57:02 +0100
To:
Sebastian HÃ¶hna <sebastian.hoehna@gmail.com>, John Huelsenbeck <johnh@berkeley.edu>, Maxim Teslenko <maxkth@gmail.com>

Hi:

A short (!) report on the RevBayes progress I have made in the branches/fredrik branch the last two months.

1. The testParser main now compiles cleanly and runs in Windows (Visual Studio) and Mac OS Unix environment (with cmake). You will get a console window from which you can have simple statements parsed and variables inserted in the user workspace. There is a quit() or q() function that lets you quit the program. There is an ls() function that allows you to list the content of the user workspace (or the global as well if you give 'ls(all=true)'). You can define models using unif, norm and exp distribution functions and then create a deep copy of the model with the model() function. You can also create an mcmc analysis using the mcmc() command and run mcmc analyses using the member function .update() of the mcmc object you create. There is a source() command that allows you to process code that you put in a file. A command line argument will be interpreted as a file name and fed to the source() function. There is a test.txt file that reproduces the hierarchical normal model Sebastian and John were working on previously; I attach this file FYI. The parser is of course not as polished or robust as it should be yet. Lots of functions, in particular, are missing, so you cannot yet parse any expressions (like a*b + c), unfortunately. The bison code seems to do its job correctly on such expressions but the relevant function(s) will be reported as missing from the workspace.

2. If you want to see what the parser is doing, there is a number of magic PRINTF statements in various parser-related classes, controlled by a #define in Parser.h. You will get a lot of output if you choose to define DEBUG_PARSER. We probably want to have this partitioned into various component sets of print statements eventually. Only the root of the syntax tree is printed currently before evaluation / execution of the statement. I need to change that so the entire tree is printed.

3. The for loop does not work properly. It seems like I have a glitch in the syntax specification for the multiple statements that should go in the loop, the first real glitch I have found so far in the grammar files. It should not be too serious but it will take me some time to figure out what is going on and I can no longer prioritize RevBayes the way I have done the last two months, I simply have too many other things to catch up with now, so it will take some time before I have this fixed. For now, I am using the group assignment to vectors proposed by John. You can see how it works in test.txt. Of course, Sebastian is opposed to anything that would allow the user to express in one line what you can do in several lines in a for loop, so he is not happy about this. For now, I guess the alternative for Sebastian is to spell out all statements that were supposed to be executed in the for loop. (Or you can program the for loop yourselves if you are interested). Sorry about this, it is the best I can do right now. When I have the for loop working, I essentially have all important types of language components implemented, it is only a matter of filling in the details after that. The for loop is just one example of the statement processing that needs to go into the user-defined functions.

4. I have introduced a number of changes that I think are uncontroversial but I have two important things we need to discuss. The first is the RbFunction interface. John suggested that using a workspace and giving away only pointers to a variable inside each function would speed up the update of the DAG during MCMC. In fact, this model causes lots of problems, including some unanticipated ones.

First, I want to explain why the parser really needs to have independent values (copies) produced by the functions instead of pointers to variables inside the functions. Consider for instance a statement like a*b + c*d. Evaluating this would include two calls to the multiplication function and one to the addition function. With the current model, when the addition function asks for the values of its operands, it will get pointers to the same value from the multiplication function, so the result will be 2*a*b or 2*c*d depending on the order of calling the multiplication function, unless the addition function makes or gets independent copies of the operand values (or the parser uses two independent copies of the multiplication function).

Second, consider what happens in each parser function call with the current model. The function would typically calculate a new value and assign it to its workspace variable first. In the general case, this might involve a call to the assignment operator of the workspace object. After writing a few assignment operators, you realize that this code is essentially the same as the code in the destructor plus the code in the constructor, including nested calls to all base class assignment operators. In the parser, as explained above, there is a necessary copy constructor call on top of this. So, the parser actually uses the equivalent, more or less, of three calls to destructors/constructors for each value it calculates.

Third, it becomes really ugly when you deal with values like RbComplex objects containing DAG nodes as member variables, like distribution objects. For instance, when you construct a stochastic node, you might want to call a constructor function for the appropriate distribution first to evaluate the right-hand-side expression (e.g., 'norm(mu,sigma)' of the expression 'x ~ norm(mu, sigma)'). If the constructor function uses a local working copy of the distribution, that working copy gets hooked up to the DAG in the process. Now, when you create the stochastic node ('x') in the second step, you have to make an independent copy of the distribution. You end up with both the copy ('x') and the constructor function's working copy hooked up to the DAG. To remove the inappropriate working copy from the DAG, you need to somehow tell the function to erase its working copy after you have made the independent copy. Not very elegant if you ask me.

All these problems will be solved if our functions return independent value copies on each call. I suggest we move in this direction. Currently, I have two interfaces in RbFunction, the old one and a new one called getValue, which returns an independent copy instead of a pointer to an internal value. I currently call getValue in the parser. Of course, this is a temporary fix that is only there for discussion. If we wanted to keep the model with two types of function calls, all functions returning RbComplex values, minimally, would have to be implemented separately in two different functions, one with an internal working copy and one giving away a new copy of the calculated value.

5. I am still thinking about the best way of implementing types and type conversions. I currently use argument rules in setting member variables. This gives very precise control over the values used in setting the variables. However, once you start running an MCMC analysis on the DAG, or a simulation, the only thing you can be guaranteed is that the value of the variable is always of the same type. Thus, if we want the sd of the normal distribution, e.g., to always be a positive value, we need to introduce a type for positive real values (or accept that errors might be thrown, or invent a more complex mechanism for predicting the range of values we might see). I have illustrated the separate type approach in my branch by introducing a PosReal class and I use it for the value type of the exponential function and for the parameter type of the sd variable of the uniform function. It is perhaps not exactly right but it illustrates the idea. Furthermore, you should know that when the user creates a real value (e.g. 'x <- 1.0'), it is assumed to be of type PosReal if it is positive, else it is assumed to be a double ('x <- 0.0' would create a double and not a positive real, for instance). For this model to work well, we need some implicit type conversions, but I would like to have your opinions on this before proceeding so I have not added this yet.

6. I have checked covariant return types thoroughly. They have been in the ANSI C++ standard since 1998, which is essentially an eternity. After I discovered that you were using them a couple of places in code that you were obviously running successfully I started using them consistently in the dag classes and sometimes elsewhere as well. The code compiles and runs fine with the covariant return types both with gcc and the microsoft compiler. If you can show me a convincing example of a non-archaic compiler or IDE that cannot handle co-variant return types, I promise I will change all of them back to non-variant return types.

7. I am still not quite sure about the best way of handling references (often called wrappers by me) to variables. There is currently a lookup function that is inserted when there is a reference; in principle, this will allow us to always do the things you expect you might be able to do with a reference to a variable. In DAGs, however, the lookups are not strictly necessary when the reference is to a simple named variable in a distribution. It is possible to strip the lookups out of the DAG in these cases, if we think it is desirable. The lookup nodes should not be a problem for the MCMC analysis, however, the DAG still does the right thing.

8. I implemented a simple reference counting mechanism for memory management. Essentially, an object gets inserted inside a DAGNode, which gets inserted into a frame or one of a few similar types of receptacles managing the memory. The receptacle names the DAG node and the number of references to the node is simply the number of children plus the receptacle naming it. We probably want to do something more sophisticated eventually but this should work for now.

9. I prefer using NULL for undefined values. NULL can be used in all contexts in contrast to an undefined object. It is true that an undefined object can save you from a crash on a NULL pointer, but in almost all cases, such a crash means you have forgotten that you may deal with undefined values, so I prefer to have an easily fixed NULL pointer crash to more spurious problems caused by an inappropriately handled undefined object. You can find some other of my coding practices defined in coding_practices.txt in the documentation folder.

10. I have deleted some redundant code or code that needs to be rewritten in my branch called fredrik but there is still a fair amount of such code even in that branch.

11. I presented a fair amount of information in previous mails about how I redesigned the DAG nodes, how I moved monitors, moves and other MCMC functionality from the DAG and the Model classes to the Mcmc class, etc. I am not going to reiterate those points here.

12. I encourage you to play with the code, modify it and build things on top of what I have now but please do that in other parts of the svn repository, leave my current branch intact. I would like to keep it for future reference; eventually it will go. I am slowly getting a little more sophisticated with svn. I recently found out how you actually should start new branches (using svn copy), and how you can merge them back into the trunk (using two different variants of svn merge). The current fredrik branch was not started the right way, so it does not have the proper svn revision history information.  Beware of this if you wish to merge it into other branches or into the trunk.


Have fun,

Fredrik


test.txt


# Test file for mcmc on normal distribution

a <- -1.0
b <- 1.0

mu ~ unif(a, b)
sigma ~ exp(1.0)

# Testing group assignment; loops do not work yet
x[10] <- 0.0;
x ~ norm(mu, sigma);
x <- 0.5;

mymodel <- model(x)

mymcmc <- mcmc(mymodel, ngen=100, printfreq=1, samplefreq=1, file="test.out")

mymcmc.update()